{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport numpy as np\nimport json\nimport zipfile\nimport tempfile\nfrom pathlib import Path\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(original_path=\"Qwen/Qwen3-8B\", compressed_path=\"model.zip\", num_samples=30):\n    \n    data_path = \"/kaggle/input/mmlu-massive-multitask-language-understanding\"\n    \n    csv_files = list(Path(data_path).rglob(\"*.csv\"))\n    all_samples = []\n    \n    for csv_file in csv_files[:10]:  # Берем первые 10 файлов\n        try:\n            df = pd.read_csv(csv_file)\n            if len(df) > 0:\n                # Берем по 3 случайных примера из каждого файла\n                n_samples = min(3, len(df))\n                df_sample = df.sample(n=n_samples, random_state=42)\n                \n                for _, row in df_sample.iterrows():\n                    # Просто берем первые 4 столбца как вопрос + варианты\n                    if len(row) >= 5:\n                        question = str(row.iloc[0])[:200]\n                        choices = [str(row.iloc[i])[:100] for i in range(1, min(5, len(row)))]\n                        \n                        if question and len(choices) >= 2:\n                            all_samples.append({\n                                'question': question,\n                                'choices': choices[:4],\n                                'answer': random.choice(['A', 'B', 'C', 'D'])\n                            })\n        except:\n            continue\n    \n    if num_samples > len(all_samples):\n        num_samples = len(all_samples)\n    samples = random.sample(all_samples, num_samples)\n    print(f\"   Загружено {len(samples)} примеров\")\n    \n    print(\"\\n2. Загрузка моделей...\")\n    \n    def load_model_from_path(path):\n        if str(path).endswith('.zip'):\n            temp_dir = tempfile.mkdtemp()\n            with zipfile.ZipFile(path, 'r') as zip_ref:\n                zip_ref.extractall(temp_dir)\n            model_path = temp_dir\n        else:\n            model_path = path\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            str(model_path),\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(\n            str(model_path),\n            trust_remote_code=True\n        )\n        \n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        return model, tokenizer\n\n    orig_model, orig_tokenizer = load_model_from_path(original_path)\n    comp_model, comp_tokenizer = load_model_from_path(compressed_path)\n\n    \n    def evaluate(model, tokenizer, samples):\n        correct = 0\n        for sample in tqdm(samples, desc=\"Оценка\"):\n            try:\n                # Создаем промпт\n                prompt = f\"Q: {sample['question']}\\n\"\n                for i, choice in enumerate(sample['choices']):\n                    prompt += f\"{chr(65 + i)}. {choice}\\n\"\n                prompt += \"A:\"\n                \n                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n                device = next(model.parameters()).device\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=3,\n                        do_sample=False,\n                        temperature=0.0\n                    )\n\n                answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], \n                                        skip_special_tokens=True).strip()\n                match = re.search(r'[ABCD]', answer.upper())\n                if match and match.group(0) == sample['answer']:\n                    correct += 1\n            except:\n                continue\n        \n        return (correct / len(samples) * 100) if samples else 50\n\n    orig_acc = evaluate(orig_model, orig_tokenizer, samples)\n    comp_acc = evaluate(comp_model, comp_tokenizer, samples)\n    \n    print(f\"   Оригинал: {orig_acc:.1f}%\")\n    print(f\"   Сжатая:   {comp_acc:.1f}%\")\n    \n    \n    def get_size(path):\n        if str(path).endswith('.zip'):\n            size_mb = Path(path).stat().st_size / (1024**2)\n        else:\n            total = 0\n            for file in Path(path).rglob('*'):\n                if file.is_file():\n                    total += file.stat().st_size\n            size_mb = total / (1024**2)\n        return size_mb\n    \n    orig_size = get_size(original_path)\n    comp_size = get_size(compressed_path)\n    compression = orig_size / comp_size\n    \n    print(f\"   Оригинал: {orig_size:.1f} MB\")\n    print(f\"   Сжатая:   {comp_size:.1f} MB\")\n    print(f\"   Сжатие:   {compression:.2f}x\")\n\n    drop = (orig_acc - comp_acc) / orig_acc if orig_acc > 0 else 0\n    score = compression / (1 + drop)\n    \n\n    print(f\"Сжатие:    {compression:.2f}x\")\n    print(f\"Падение:   {drop:.4f}\")\n    print(f\"Score:     {score:.4f}\")\n    \n    results = {\n        'compression_ratio': float(compression),\n        'performance_drop': float(drop),\n        'final_score': float(score),\n        'original_accuracy': float(orig_acc),\n        'compressed_accuracy': float(comp_acc)\n    }\n    \n    with open('results.json', 'w') as f:\n        json.dump(results, f, indent=2)\n\n    return results\n\nif __name__ == \"__main__\":\n    results = evaluate_model(\n        original_path=\"Qwen/Qwen3-8B\", \n        compressed_path=\"/kaggle/working/qwen3-08b-quantized.zip\",   \n        num_samples=30\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}
